% Since it can be hard to visualize what the Backpropagation Algorithm is doing% It's good to have a way to check wheather it's working or not% A good way to do this can be by using the following idea: if the backprop is% working then the value of the cost function is decreasing and so is it's% gradients, so we calculate these an APROXIMATION of the gradient values% using something similar to the definition of derivatives.  That is, we% approximate the derivative of J(theta) - which is now a matrix instead of a% real number (since theta is a matrix instead of a vector) - with:%                      (J(theta1, ...., thetaI + epsilon, ..., thetaN) - J(theta1, ..., thetaI - epsilon, ...., thetaN) ) / 2epsilon% This algorithm looks like this:epsilon = 10^-4;for i = 1:n  thetaPlus = theta;  thetaPlus(i) += epsilon;    thetaMinus = theta;  thetaMinus(i) -= epsilon;    gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) / (2*epsilon);end% Obs. 1: epsilon has to be a small value so the calculation approximates the% derivative well, however it cannot be to small because of numerical issues% Obs.2 (IMPORTANT): do not run this checking for every iteration of the% backprop algorithm, only the first one, since this checking can very% expensive computationally